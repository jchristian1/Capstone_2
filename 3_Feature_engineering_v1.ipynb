{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f465016-721c-4cc6-8853-46e0f579be86",
   "metadata": {},
   "source": [
    "# Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68aacb-dfb5-4399-abc9-7192d0e415b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c22a52-ea1b-4362-94ff-e1c5e6f91d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff4f14-6cea-4519-8560-14137c19d1bc",
   "metadata": {},
   "source": [
    "# 1. Feature Engineering: Calculating Yearly S&P 500 Returns\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this section of our project, we focus on feature engineering, specifically on deriving the yearly returns of the S&P 500 index. These returns will serve as the dependent variable in our analysis, helping us assess the predictive power of financial metrics from preceding years on future stock performance relative to the market.\n",
    "\n",
    "## Objective\n",
    "\n",
    "The objective of this feature engineering phase is to:\n",
    "- Calculate the yearly returns (percentage price change) for the S&P 500 for each year from 2015 to 2019.\n",
    "- Use these returns as the dependent variable to analyze the correlation between past financial indicators and the subsequent year's stock performance relative to the S&P 500.\n",
    "\n",
    "## Data Acquisition\n",
    "\n",
    "We will use the `yfinance` library to download historical data for the S&P 500 index (`^GSPC`). The period of interest spans from December 31, 2014, to December 31, 2019, allowing us to calculate annual returns that are synchronized with the financial year-ends of the stocks under study.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. **Downloading Data**: Fetch the historical adjusted close prices for the S&P 500 from Yahoo Finance using the `yfinance` library.\n",
    "2. **Annual Returns Calculation**:\n",
    "   - Resample the data to obtain the closing price on the last trading day of each year.\n",
    "   - Calculate the percentage change in these closing prices year-over-year to derive the annual returns.\n",
    "3. **Alignment with Financial Metrics**:\n",
    "   - Align these annual returns with the financial data from the end of each previous year (2014-2018). This alignment ensures that each return value from 2015 to 2019 corresponds to financial metrics from the end of 2014 to 2018, respectively.\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "This feature engineering step will provide us with a well-defined dependent variable, representing the S&P 500 yearly returns. By correlating these returns with the financial metrics of stocks from the previous years, we aim to uncover significant predictors of stock performance relative to the broader market.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The successful calculation and integration of yearly S&P 500 returns into our dataset are crucial for subsequent predictive modeling. This step forms the backbone of our analysis, enabling us to rigorously test our hypothesis that prior financial metrics can serve as reliable indicators of future market outperformance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688fde3c-fff0-4e5b-9af5-dc2593ba9c92",
   "metadata": {},
   "source": [
    "#### Fetch the S&P 500 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac33a92-eb8c-4bec-b5f1-25fd6643025c",
   "metadata": {},
   "source": [
    "Note: The yfinance library does use adjusted closing prices from Yahoo Finance, which accounts for corporate actions like dividends and stock splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6945418-f58a-4c99-b4ed-8b41ea175478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ticker symbol for the S&P 500\n",
    "ticker_symbol = \"^GSPC\"\n",
    "\n",
    "# Set the start and end dates for each year-end closing from 2014 to 2018\n",
    "start_date = \"2014-12-31\"\n",
    "end_date = \"2023-12-31\"\n",
    "\n",
    "# Download the historical data\n",
    "data = yf.download(ticker_symbol, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate the yearly close by resampling to annual frequency at year-end\n",
    "# The 'Y' frequency here will ensure that it picks the last trading day of each calendar year\n",
    "yearly_data = data['Adj Close'].resample('YE').last()\n",
    "\n",
    "# Calculate the yearly returns\n",
    "sp_yearly_returns = yearly_data.pct_change().dropna() * 100\n",
    "\n",
    "# The yearly returns should not be offset by -1 year in the index for your needs,\n",
    "# because we want the financial year data of 2014 (ending in 2014) to predict the gains in 2015, and so on\n",
    "# The index should just be adjusted to correspond to the years where the returns are applicable\n",
    "sp_yearly_returns.index = sp_yearly_returns.index.year\n",
    "\n",
    "# Print the yearly returns\n",
    "sp_yearly_returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361788f-475d-4b7c-b39c-37f40cec3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sp_yearly_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc0a3ea-9e32-4051-a452-fd6199f3a72b",
   "metadata": {},
   "source": [
    "## Calculation of Stock Alphas Relative to S&P 500\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this section, we will calculate the alpha for each stock in our dataset for the years 2015 to 2018. Alpha measures the performance of a stock relative to a benchmark, which in this case is the S&P 500. A positive alpha indicates that the stock has outperformed the benchmark on a risk-adjusted basis, while a negative alpha indicates underperformance.\n",
    "\n",
    "### Objective\n",
    "\n",
    "The objective of this analysis is to determine how much each stock over- or under-performed relative to the S&P 500. This insight can help identify stocks that might offer better returns compared to the market.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "We calculate alpha using the formula:\n",
    "alpha = {Stock Return} - {S&P 500 Return}\n",
    "Where:\n",
    "- **Stock Return** is the percentage price variation of the stock for the year.\n",
    "- **S&P 500 Return** is the return of the S&P 500 index for the corresponding year.\n",
    "\n",
    "### Execution\n",
    "\n",
    "We will add a new column for alpha for each year from 2014 to 2018, using the yearly returns of the S&P 500 and the stock's performance from the previous year's dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360057f2-268d-4067-925c-c21bcd305622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "data_2014 = pd.read_csv('processed_data/clean_df_2014.csv')\n",
    "data_2015 = pd.read_csv('processed_data/clean_df_2015.csv')\n",
    "data_2016 = pd.read_csv('processed_data/clean_df_2016.csv')\n",
    "data_2017 = pd.read_csv('processed_data/clean_df_2017.csv')\n",
    "data_2018 = pd.read_csv('processed_data/clean_df_2018.csv')\n",
    "\n",
    "# Add alpha calculation for each dataset\n",
    "datasets = [data_2014, data_2015, data_2016, data_2017, data_2018]\n",
    "year_start = 2015\n",
    "\n",
    "def create_alphas(datasets, yearly_returns, year_start):\n",
    "    \"\"\"\n",
    "    Updates the datasets with alpha values computed as the difference between \n",
    "    the price variation and yearly returns of the S&P 500.\n",
    "\n",
    "    Parameters:\n",
    "    - datasets: List of pandas DataFrames containing the yearly data.\n",
    "    - yearly_returns: Dictionary containing yearly returns of the S&P 500.\n",
    "    - year_start: The starting year for the datasets.\n",
    "    \n",
    "    Returns:\n",
    "    - updated_datasets: List of updated DataFrames with alpha columns added.\n",
    "    \"\"\"\n",
    "    updated_datasets = []\n",
    "\n",
    "    for i, data in enumerate(datasets):\n",
    "        price_var_col = f'{year_start + i} PRICE VAR [%]'\n",
    "        alpha_col = f'Alpha_{year_start + i}'\n",
    "        \n",
    "        if price_var_col in data.columns:\n",
    "            data[alpha_col] = data[price_var_col] - yearly_returns[year_start + i]\n",
    "        \n",
    "        # Append the updated DataFrame to the list\n",
    "        updated_datasets.append(data)\n",
    "        \n",
    "        # Print the updated DataFrame to verify the new alpha column\n",
    "        print(data[['Symbol', price_var_col, alpha_col]].head(1))\n",
    "    \n",
    "    return updated_datasets\n",
    "\n",
    "# Example usage\n",
    "datasets = [data_2014, data_2015, data_2016, data_2017, data_2018]\n",
    "\n",
    "updated_datasets = create_alphas(datasets, sp_yearly_returns, year_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624ede6d-62aa-42cf-b1ba-1127ab6e5bf4",
   "metadata": {},
   "source": [
    "### Let's check everything went well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2eb19c-a1b6-4fcb-88b5-edf1a9b5ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Alpha first stock in the dataset 2014: {data_2014['2015 PRICE VAR [%]'][0]-sp_yearly_returns[2015]}',\\\n",
    "f'Alpha first stock in the dataset 2015: {data_2015['2016 PRICE VAR [%]'][0]-sp_yearly_returns[2016]}',\\\n",
    "f'Alpha first stock in the dataset 2016: {data_2016['2017 PRICE VAR [%]'][0]-sp_yearly_returns[2017]}',\\\n",
    "f'Alpha first stock in the dataset 2017: {data_2017['2018 PRICE VAR [%]'][0]-sp_yearly_returns[2018]}',\\\n",
    "f'Alpha first stock in the dataset 2018: {data_2018['2019 PRICE VAR [%]'][0]-sp_yearly_returns[2019]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2396b433-3018-45a8-a56f-24bee0a260de",
   "metadata": {},
   "source": [
    "It looks like everything is correct, now it would be interesting to create another dependent variables for the gains of 3 and 5 years for future modeling. But we could come back to do it once we finish the first modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f32302-ee87-43e5-a88a-f5b33b371273",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "575fe33d-eb76-4b82-a134-4029b06a078e",
   "metadata": {},
   "source": [
    "## Visualizing Stocks Outperforming the S&P 500\n",
    "\n",
    "### Let's visualize our findings using the dependent variable\n",
    "\n",
    "We will visualize the stocks that have outperformed the S&P 500 by year. Specifically, we will look at the top and bottom performers in terms of alpha, which measures the performance of each stock relative to the S&P 500. \n",
    "\n",
    "### Function Definition\n",
    "\n",
    "The `plot_alphas` function is designed to plot the alphas for a given year, highlighting the top and bottom performers. The function takes the following parameters:\n",
    "- `data`: The dataset containing the stock information and calculated alphas.\n",
    "- `year`: The year for which to plot the alphas.\n",
    "- `top_n`: The number of top-performing stocks to display (default is 10).\n",
    "- `bottom_n`: The number of bottom-performing stocks to display (default is 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa1845-b3a0-42d0-8d26-7241840733e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot alphas for a given year with parameter for top and bottom records\n",
    "def plot_alphas(data, year, top_n=10, bottom_n=10):\n",
    "    alpha_col = f'Alpha_{year}'\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Sort data by alpha\n",
    "    data_sorted = data.sort_values(by=alpha_col, ascending=False)\n",
    "    \n",
    "    # Select top and bottom performers\n",
    "    top_bottom = pd.concat([data_sorted.head(top_n), data_sorted.tail(bottom_n)])\n",
    "    \n",
    "    sns.barplot(x='Symbol', y=alpha_col, data=top_bottom)\n",
    "    plt.title(f'Stocks Outperforming S&P 500 in {year}')\n",
    "    plt.xlabel('Stock Symbol')\n",
    "    plt.ylabel('Alpha')\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True, axis='y', linestyle='--', linewidth=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# Plot alphas for each year with default top 10 and bottom 10\n",
    "plot_alphas(data_2014, 2015, top_n=10, bottom_n=10)\n",
    "plot_alphas(data_2015, 2016, top_n=10, bottom_n=10)\n",
    "plot_alphas(data_2016, 2017, top_n=10, bottom_n=10)\n",
    "plot_alphas(data_2017, 2018, top_n=10, bottom_n=10)\n",
    "plot_alphas(data_2018, 2019, top_n=10, bottom_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf160140-fb35-4bab-9c08-63b6227a6239",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_of_interest = ['VLRS', 'RARE']\n",
    "filtered_data = data_2014[data_2014['Symbol'].isin(symbols_of_interest)][['Alpha_2015', '2015 PRICE VAR [%]']]\n",
    "print(filtered_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf378cf-ce6c-4b81-b587-e3ae33963457",
   "metadata": {},
   "source": [
    "#### This looks weird, Why we have a lot of different stocks with the same price variation? let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce9d44-24c4-44fd-93c7-8981e5a8017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2014.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58da543-6fb6-43b7-ac7a-1216f0a4e9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2014['2015 PRICE VAR [%]'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c5351b-bc34-4302-85a9-0581680a27c7",
   "metadata": {},
   "source": [
    "We have a lot of stocks with the same performance could be that correct? i do not think so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd3fe8-1d39-4607-9187-c9b391a489aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the non-unique values in the '2015 PRICE VAR [%]' column\n",
    "non_unique_values = data_2014['2015 PRICE VAR [%]'][data_2014['2015 PRICE VAR [%]'].duplicated()].unique()\n",
    "\n",
    "# Filter the dataset for those non-unique values and get the symbols\n",
    "non_unique_symbols = data_2014[data_2014['2015 PRICE VAR [%]'].isin(non_unique_values)]['Symbol']\n",
    "\n",
    "# Print the unique symbols associated with non-unique values\n",
    "print(non_unique_symbols.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb71a0eb-0a97-4a0c-b288-7cd1e5f89ed8",
   "metadata": {},
   "source": [
    "#### Let's recalculate the price variations for each non unique stock, fetching historical data from yfinance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b480c3-bf0b-4bed-800e-a019a7898565",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_variations = {}\n",
    "failed_symbols = []\n",
    "\n",
    "# Loop through each symbol to fetch data and calculate price variation\n",
    "for symbol in non_unique_symbols:\n",
    "    try:\n",
    "        # Fetch historical data for 2015\n",
    "        data = yf.download(symbol, start='2015-01-01', end='2016-01-01')\n",
    "        \n",
    "        # Calculate price variation if data is available\n",
    "        if not data.empty:\n",
    "            price_start = data['Close'].iloc[0]  # Price at the start of 2015\n",
    "            price_end = data['Close'].iloc[-1]   # Price at the end of 2015\n",
    "            price_variation = ((price_end - price_start) / price_start) * 100  # Percentage variation\n",
    "            price_variations[symbol] = price_variation\n",
    "        else:\n",
    "            failed_symbols.append(symbol)\n",
    "    except Exception as e:\n",
    "        failed_symbols.append(symbol)\n",
    "\n",
    "# Convert to DataFrame for easier viewing\n",
    "price_variations_df = pd.DataFrame(price_variations.items(), columns=['Symbol', '2015 Price Variation [%]'])\n",
    "\n",
    "# Print results\n",
    "price_variations_df\n",
    "\n",
    "# Print failed symbols\n",
    "if failed_symbols:\n",
    "    print(\"Failed to download data for the following symbols:\")\n",
    "    failed_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8081037-907f-4768-b6b1-8fe3bc8e1b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "price_variations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823689ea-8599-42a1-9697-44dbd98c39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(non_unique_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af0ed8-18c2-433b-8234-4f51fa80f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(failed_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3808f29-1b79-4150-a3a8-78492f32a5f3",
   "metadata": {},
   "source": [
    "#### We are missing the price variation for 94 different companies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a2a662-c44a-4289-95bf-1b82e472e24b",
   "metadata": {},
   "source": [
    "I checked manually on the web for some of the variations and it looks good. Let's drop the observations we could not fetch and add the new values to the existing datasets. We are going to create a function to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc105d9b-9e16-40c6-acf1-17d93adece3c",
   "metadata": {},
   "source": [
    "## Calculation of Price Variations Using Historical Data\n",
    "\n",
    "### Purpose\n",
    "The primary goal of this function is to calculate and update the price variations for stocks based on historical data. This process helps enhance our datasets by replacing non-unique values with actual price variations retrieved from reliable financial sources.\n",
    "\n",
    "### Background\n",
    "In financial analysis, understanding stock price movements is crucial for making informed investment decisions. The datasets we are working with contain annual price variation metrics, which we aim to validate and update using historical stock price data. \n",
    "\n",
    "#### Dataset Structure\n",
    "Each dataset corresponds to a year and includes:\n",
    "- **Symbol**: The ticker symbol of the stock.\n",
    "- **[Year] PRICE VAR [%]**: The price variation percentage for the following year (e.g., the 2014 dataset has a `2015 PRICE VAR [%]`).\n",
    "\n",
    "### Process Overview\n",
    "1. **Identify Non-Unique Values**: For each dataset, we identify non-unique values in the `PRICE VAR [%]` column. This helps pinpoint stocks where the variation data may be unreliable or needs verification.\n",
    "  \n",
    "2. **Fetch Historical Price Data**: Using the `yfinance` library, we retrieve the historical closing prices for each stock for the year after the dataset year. For example, for the 2014 dataset, we fetch data for 2015.\n",
    "\n",
    "3. **Calculate Price Variations**: We compute the percentage change in stock price using the formula:\n",
    "   \\[\n",
    "   \\text{Percentage Change} = \\left(\\frac{\\text{Final Price} - \\text{Initial Price}}{\\text{Initial Price}}\\right) \\times 100\n",
    "   \\]\n",
    "\n",
    "4. **Update DataFrames**: The calculated price variations replace the non-unique values in the respective `PRICE VAR [%]` columns. \n",
    "\n",
    "5. **Handle Failed Fetches**: Stocks that could not be fetched (e.g., due to being delisted or unavailable data) are recorded and removed from the dataset to ensure data integrity.\n",
    "\n",
    "6. **Summary of Changes**: A summary is generated for each year, indicating:\n",
    "   - The total number of non-unique symbols.\n",
    "   - How many variations were updated.\n",
    "   - How many symbols were dropped due to failed data retrieval.\n",
    "\n",
    "### Conclusion\n",
    "This systematic approach ensures that our datasets remain accurate and reflective of real-world market conditions. By regularly updating and verifying stock price data, we enhance the reliability of our financial analyses and predictive models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d56d9-565c-44c3-86fe-8954cdac4ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def compute_price_variations(datasets, year_start):\n",
    "    \"\"\"\n",
    "    Updates the datasets with price variations computed from historical data and provides a summary.\n",
    "\n",
    "    Parameters:\n",
    "    - datasets: List of pandas DataFrames containing the yearly data.\n",
    "    - year_start: The starting year for the datasets.\n",
    "    \n",
    "    Returns:\n",
    "    - updated_datasets: List of updated DataFrames with new price variations added.\n",
    "    - summary: Dictionary summarizing the updates.\n",
    "    \"\"\"\n",
    "    summary = {}\n",
    "\n",
    "    for i, df in enumerate(datasets):\n",
    "        year = year_start + i\n",
    "        price_var_col = f'{year + 1} PRICE VAR [%]'\n",
    "        \n",
    "        # Identify non-unique values for the corresponding year\n",
    "        non_unique_values = df[price_var_col][df[price_var_col].duplicated()].unique()\n",
    "        non_unique_symbols = df[df[price_var_col].isin(non_unique_values)]['Symbol'].unique()\n",
    "        \n",
    "        # Create a dictionary to store price variations\n",
    "        price_variations = {}\n",
    "        failed_symbols = []\n",
    "\n",
    "        for symbol in non_unique_symbols:\n",
    "            try:\n",
    "                # Fetch historical data for the year of interest\n",
    "                data = yf.download(symbol, start=f'{year + 1}-01-01', end=f'{year + 2}-01-01')\n",
    "                \n",
    "                if not data.empty:\n",
    "                    price_start = data['Close'].iloc[0]  # Price at the start of the year\n",
    "                    price_end = data['Close'].iloc[-1]   # Price at the end of the year\n",
    "                    price_variation = ((price_end - price_start) / price_start) * 100  # Percentage variation\n",
    "                    price_variations[symbol] = price_variation\n",
    "                else:\n",
    "                    failed_symbols.append(symbol)\n",
    "            except Exception:\n",
    "                failed_symbols.append(symbol)\n",
    "\n",
    "        # Update the DataFrame with new variations\n",
    "        for symbol, variation in price_variations.items():\n",
    "            df.loc[df['Symbol'] == symbol, price_var_col] = variation\n",
    "        \n",
    "        # Drop rows with symbols that couldn't be fetched\n",
    "        df.drop(df[df['Symbol'].isin(failed_symbols)].index, inplace=True)\n",
    "\n",
    "        # Summary of changes\n",
    "        summary[year] = {\n",
    "            'Total Non-Unique Symbols': len(non_unique_symbols),\n",
    "            'Updated Variations': len(price_variations),\n",
    "            'Dropped Symbols': len(failed_symbols)\n",
    "        }\n",
    "    \n",
    "    return datasets, summary\n",
    "\n",
    "# Run compute price variations\n",
    "dataframes_list = [data_2014, data_2015, data_2016, data_2017, data_2018]\n",
    "year_start = 2014\n",
    "\n",
    "updated_dataframes, summary = compute_price_variations(dataframes_list, year_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb069c10-11fa-43e4-ba77-ffd1c90fc21b",
   "metadata": {},
   "source": [
    "#### Now we are going to use the create alphas function to update the column using the new data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e92defb-410d-4fcc-99f3-d827afd5662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_datasets = create_alphas(updated_dataframes, sp_yearly_returns, 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb715cf0-2d94-4371-923e-eaf900be7b80",
   "metadata": {},
   "source": [
    "#### Let's plot the alphas again to see how they look now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b9615-49af-4f4d-bbaa-2f476848ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot alphas for each year with default top 10 and bottom 10\n",
    "top_n = 40\n",
    "bottom_n = 8\n",
    "\n",
    "plot_alphas(data_2014, 2015, top_n=top_n, bottom_n=bottom_n)\n",
    "plot_alphas(data_2015, 2016, top_n=top_n, bottom_n=bottom_n)\n",
    "plot_alphas(data_2016, 2017, top_n=top_n, bottom_n=bottom_n)\n",
    "plot_alphas(data_2017, 2018, top_n=top_n, bottom_n=bottom_n)\n",
    "plot_alphas(data_2018, 2019, top_n=top_n, bottom_n=bottom_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b80707-7b0f-4409-9523-3f312b0734b5",
   "metadata": {},
   "source": [
    "#### Just for curiosity let's see how many stocks outperform the S&P500 in the year of study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6797b0c-c039-4d86-a8be-dc6dd66c0730",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2014[data_2014['2015 PRICE VAR [%]']>0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3125fc9b-51fe-41a0-a109-817c4da872bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2015[data_2015['2016 PRICE VAR [%]']>0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cece30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2016[data_2016['2017 PRICE VAR [%]']>0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4999eed9-fda7-4365-98aa-281e19deab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2017[data_2017['2018 PRICE VAR [%]']>0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056bd0ab-543f-46cb-9357-8adf889f2e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2018[data_2018['2019 PRICE VAR [%]']>0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41699863-fc89-4753-b334-557099d5870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data_2014.columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2133017-ef85-4459-bb4d-1b7ab15c6382",
   "metadata": {},
   "source": [
    "# 2. Dummie variables, Scaling and dataset split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4363435-3c3b-452a-8b7b-4e06f38c2cf5",
   "metadata": {},
   "source": [
    "Let's change the column name 2014 price var and alpha_2014 for something that contain all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89b7a98-2f6b-44e0-b4f7-1b750d53c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [data_2014, data_2015, data_2016, data_2017, data_2018]\n",
    "\n",
    "# Rename columns dynamically based on the year\n",
    "for i, df in enumerate(datasets):\n",
    "    year = 2014 + i\n",
    "    df.rename(columns={f'{year + 1} PRICE VAR [%]': 'PRICE VAR [%]', f'Alpha_{year + 1}': 'Alpha'}, inplace=True)\n",
    "    \n",
    "data_2015.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff874d1-5104-4891-9e0a-0f3667234236",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e800078f-516c-4433-9212-7866fc3843ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets for processing\n",
    "combined_data = pd.concat(datasets).reset_index(drop=True)\n",
    "\n",
    "# Create dummy features for the 'Sector' column\n",
    "combined_data = pd.get_dummies(combined_data, columns=['Sector'], drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "features = combined_data.drop(columns=['Alpha', 'Symbol'])\n",
    "target = combined_data['Alpha']\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "features_scaled_df = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "\n",
    "# Add the target column back to the scaled features DataFrame\n",
    "features_scaled_df['Alpha'] = target.reset_index(drop=True)\n",
    "\n",
    "# Reset indices to ensure alignment for boolean indexing\n",
    "features_scaled_df = features_scaled_df.reset_index(drop=True)\n",
    "combined_data = combined_data.reset_index(drop=True)\n",
    "\n",
    "# Split the data into training (2014-2016), validation (2017), and test (2018) sets\n",
    "train_data = features_scaled_df[combined_data['Year'].isin([2014, 2015, 2016])]\n",
    "validation_data = features_scaled_df[combined_data['Year'] == 2017]\n",
    "test_data = features_scaled_df[combined_data['Year'] == 2018]\n",
    "\n",
    "# Check if any of the splits are empty\n",
    "print(\"Training set shape:\", train_data.shape)\n",
    "print(\"Validation set shape:\", validation_data.shape)\n",
    "print(\"Test set shape:\", test_data.shape)\n",
    "\n",
    "# Extract features and target for each set\n",
    "X_train = train_data.drop(columns=['Alpha'])\n",
    "y_train = train_data['Alpha']\n",
    "\n",
    "X_validation = validation_data.drop(columns=['Alpha'])\n",
    "y_validation = validation_data['Alpha']\n",
    "\n",
    "X_test = test_data.drop(columns=['Alpha'])\n",
    "y_test = test_data['Alpha']\n",
    "\n",
    "# Ensure splits are not empty before standardizing\n",
    "if not X_train.empty and not X_validation.empty and not X_test.empty:\n",
    "    # Standardize the training, validation, and test features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_validation_scaled = scaler.transform(X_validation)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Print shapes to verify\n",
    "    print(\"Training set shape after scaling:\", X_train_scaled.shape)\n",
    "    print(\"Validation set shape after scaling:\", X_validation_scaled.shape)\n",
    "    print(\"Test set shape after scaling:\", X_test_scaled.shape)\n",
    "else:\n",
    "    print(\"One of the data splits is empty. Please check the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9865bd9-89b1-4316-861b-092cf4aba909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training, validation, and test datasets\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_validation_scaled_df = pd.DataFrame(X_validation_scaled, columns=X_validation.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Add the target variable back to each set\n",
    "X_train_scaled_df['Alpha'] = y_train.reset_index(drop=True)\n",
    "X_validation_scaled_df['Alpha'] = y_validation.reset_index(drop=True)\n",
    "X_test_scaled_df['Alpha'] = y_test.reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "X_train_scaled_df.to_csv('training_testing_validation_data/training_set.csv', index=False)\n",
    "X_validation_scaled_df.to_csv('training_testing_validation_data/validation_set.csv', index=False)\n",
    "X_test_scaled_df.to_csv('training_testing_validation_data/test_set.csv', index=False)\n",
    "\n",
    "print(\"Datasets saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ebe7c-c6f8-4199-b577-1d497ef37125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
