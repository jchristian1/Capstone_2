{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b288597a-0d5d-48e7-8c64-b37a3f61b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d75c8-a87d-477f-91fa-0ed8eaad4f7d",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac51cbfe-644a-43d7-b1c7-8186d51fcd33",
   "metadata": {},
   "source": [
    "## 1. Merging datasets of all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ebcc25-ccc7-4e6c-a7bf-583e3e52733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets from 2014 to 2018\n",
    "data_2014 = pd.read_csv('raw_data/unzipped_files/2014_Financial_Data.csv')\n",
    "data_2015 = pd.read_csv('raw_data/unzipped_files/2015_Financial_Data.csv')\n",
    "data_2016 = pd.read_csv('raw_data/unzipped_files/2016_Financial_Data.csv')\n",
    "data_2017 = pd.read_csv('raw_data/unzipped_files/2017_Financial_Data.csv')\n",
    "data_2018 = pd.read_csv('raw_data/unzipped_files/2018_Financial_Data.csv')\n",
    "# Data 2018 is already loaded as data_2018\n",
    "\n",
    "# Add a 'Year' column to each dataset before merging\n",
    "data_2014['Year'] = 2014\n",
    "data_2015['Year'] = 2015\n",
    "data_2016['Year'] = 2016\n",
    "data_2017['Year'] = 2017\n",
    "data_2018['Year'] = 2018\n",
    "\n",
    "# Combine all data into a single DataFrame\n",
    "full_df = pd.concat([data_2014, data_2015, data_2016, data_2017, data_2018], ignore_index=True)\n",
    "\n",
    "# Display the shape and first few rows of the combined dataset\n",
    "full_df.shape, full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41cb43a-4d59-407b-957e-f34ff28a07e1",
   "metadata": {},
   "source": [
    "### Rename unnamed to Symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c736ff-54f0-4ab2-a3a2-80ded2c52e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.rename(columns={'Unnamed: 0': 'Symbol'}, inplace=True)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfb1ced-f87d-45cc-909e-6f4ace079711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics after potential data cleaning or changes\n",
    "summary_after = full_df.describe()\n",
    "summary_after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a311ab79-83da-484c-b1ed-6a09d465c427",
   "metadata": {},
   "source": [
    "## Check data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76659d6e-190d-4414-b1e1-91bb9cdbd1c4",
   "metadata": {},
   "source": [
    "Everything looks good, every feature corresponds to its expected data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96893506-011f-4064-9d82-ffed16852083",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in full_df.columns:\n",
    "    print(column, full_df[column].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38803b9-8f30-4d5f-835d-9b11cd7ce5ba",
   "metadata": {},
   "source": [
    "## Consolidate price variations in one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de9bd9-bbfe-4685-9eb1-4b2f7996eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_price_var(row):\n",
    "    year_columns = {\n",
    "        2014: '2015 PRICE VAR [%]',  # Maps 2014 to the price variation in 2015\n",
    "        2015: '2016 PRICE VAR [%]',\n",
    "        2016: '2017 PRICE VAR [%]',\n",
    "        2017: '2018 PRICE VAR [%]',\n",
    "        2018: '2019 PRICE VAR [%]'\n",
    "    }\n",
    "    year = row['Year']\n",
    "    return row[year_columns[year]] if year in year_columns and not pd.isna(year) else np.nan\n",
    "\n",
    "# Apply the modified function to create a new column\n",
    "full_df['Latest PRICE VAR [%]'] = full_df.apply(get_correct_price_var, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420bdef-e414-48af-93df-91fb3856f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to drop: the yearly price variation columns\n",
    "columns_to_drop = [\n",
    "    '2015 PRICE VAR [%]', \n",
    "    '2016 PRICE VAR [%]', \n",
    "    '2017 PRICE VAR [%]', \n",
    "    '2018 PRICE VAR [%]', \n",
    "    '2019 PRICE VAR [%]'\n",
    "]\n",
    "\n",
    "# Drop these columns from the DataFrame\n",
    "full_df = full_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Verify by displaying the first few rows of the updated DataFrame\n",
    "full_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb57fae-02f5-465f-94c9-d81f4d4324c6",
   "metadata": {},
   "source": [
    "## Check features distributions before cleaning and imputations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a708845f-8d00-412e-80f1-2aa993b052fa",
   "metadata": {},
   "source": [
    "Let's create a function to plot the distribution and the boxplot of each feature. As per the dataset documentation we will drop our quantiles bellow 1% in each tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5c3ca8-4c4a-4063-9975-8517bfe908e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_financial_data(df, column, num_bins=50, outlier_cap=0.01):\n",
    "    # Handling outliers\n",
    "    lower_bound, upper_bound = df[column].quantile([outlier_cap, 1-outlier_cap])\n",
    "    df_filtered = df[(df[column] > lower_bound) & (df[column] < upper_bound)]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df_filtered[column], bins=num_bins, kde=False, color='blue')\n",
    "    plt.title(f'Histogram of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Boxplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=df_filtered[column])\n",
    "    plt.title(f'Boxplot of {column}')\n",
    "    plt.xlabel(column)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()  # Helps free up memory by closing the plot\n",
    "\n",
    "def plot_by_year(df, year):\n",
    "    # Subset data for the specific year\n",
    "    new_df = df[df['Year'] == year]\n",
    "    \n",
    "    # Select only numeric columns\n",
    "    numeric_cols = new_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    # Plot data for each numeric column\n",
    "    for column in numeric_cols:\n",
    "        plot_financial_data(new_df, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad678a6-ffc7-44fe-aa27-2d70f6dd84a7",
   "metadata": {},
   "source": [
    "### Plotting 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3945eb7b-35c0-40e4-a97b-18a72b438cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data for each numeric column in the 2014 dataset\n",
    "plot_by_year(full_df, 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af763b79-eab8-4fad-8206-53d23006169f",
   "metadata": {},
   "source": [
    "### Plotting 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae8298-48c9-4d8e-b75e-c20d837cacb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_year(full_df, 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487188f-4da3-43d1-9739-357eee43023b",
   "metadata": {},
   "source": [
    "### Plotting 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff07c3d-fa89-44ca-ab3f-5d77e9880f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_year(full_df, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b33a3-f801-471c-bffe-6a418099d1e2",
   "metadata": {},
   "source": [
    "### Plotting 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30994b2-f063-4cb0-9a47-1c530748fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_year(full_df, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72198ab6-7fa0-4a57-9a15-3167dd486013",
   "metadata": {},
   "source": [
    "### Plotting 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49690e5b-d23d-42e2-9781-d8b75c18aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_year(full_df, 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1033b7ed-ffdd-4b08-913a-8f366aba7762",
   "metadata": {},
   "source": [
    "### Now let's create a heatmap to see how features are correlated in each year (Profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a364a-a802-4a7d-bf6d-357eb90acaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    " profit_features = [\n",
    "        'Revenue', 'Gross Profit', 'Operating Income', 'Net Income',\n",
    "        'Total assets', 'Total liabilities', 'EBITDA', 'EPS'\n",
    "    ]\n",
    "\n",
    "def plot_simplified_correlation_heatmap(df, features_of_interest, year):\n",
    "    # Filter data for the specific year\n",
    "    df_year = df[df['Year'] == year]\n",
    "\n",
    "    # Select a subset of numeric columns for simplicity\n",
    "    columns_of_interest = features_of_interest\n",
    "\n",
    "    # Calculate correlation matrix for the selected columns\n",
    "    correlation_matrix = df_year[columns_of_interest].corr()\n",
    "\n",
    "    # Plotting the heatmap\n",
    "    plt.figure(figsize=(10, 8))  # Larger figure size\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "    plt.title(f'Correlation Heatmap for the Year {year}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee569c82-bf33-4cea-b2c2-b59f6bdc1ea5",
   "metadata": {},
   "source": [
    "### Heat Map 2014 (Profits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1241c6-a8fc-4499-88ca-6c97eefb7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2014\n",
    "plot_simplified_correlation_heatmap(full_df,profit_features, 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4d0957-1166-4005-967b-27b720662020",
   "metadata": {},
   "source": [
    "### Heat Map 2015 (Profits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa555a-257c-49e6-9077-8486670776be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2015\n",
    "plot_simplified_correlation_heatmap(full_df, profit_features, 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042cdd46-37f0-4132-94c0-6dabccda827f",
   "metadata": {},
   "source": [
    "### Heat Map 2016 (Profits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5683f4e-96d2-4fa1-8bd6-a0bb152dbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2016\n",
    "plot_simplified_correlation_heatmap(full_df, profit_features, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c54f3-520b-47ea-ac5a-0ddeb3648a22",
   "metadata": {},
   "source": [
    "### Heat Map 2017 (Profits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0983e3e8-6453-4083-83c2-d472d1c4a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2014\n",
    "plot_simplified_correlation_heatmap(full_df, profit_features, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f085c369-39ab-4ab1-ad63-f60f45ec1ad5",
   "metadata": {},
   "source": [
    "### Heat Map 2018 (Profits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bbcea4-88e9-476c-af3f-fe7d0adf6c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2014\n",
    "plot_simplified_correlation_heatmap(full_df, profit_features, 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f6078e-8d05-422d-9419-c53a6ff852ce",
   "metadata": {},
   "source": [
    "### Let's make histograms, boxplots for 30 metrics knowing that we have more than 200 features we will plot the ones that are more probable to be proxies to our study goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57abd50f-f015-498b-ac74-fee4db9e1f1a",
   "metadata": {},
   "source": [
    "#### Let's create a dataframe and plot features that could potentialy be the proxies to make alphas over the S&P500. (Given the amount of financial indicators and ratios we have in our dataset).\n",
    "\n",
    "##### Profitability Ratios<br><br>\n",
    "Gross Profit Margin: Measures profitability after deducting the cost of goods sold.<br>\n",
    "Operating Profit Margin: Measures profitability after deducting operating expenses.<br>\n",
    "Net Profit Margin: Measures overall profitability after all expenses, including taxes and interest.<br>\n",
    "Return on Equity (ROE): Measures the return generated on shareholders' equity.<br>\n",
    "Return on Assets (ROA): Measures the efficiency of asset utilization in generating profits.<br>\n",
    "Return on Investment (ROI): Measures the profitability of an investment relative to its cost.<br>\n",
    "##### Liquidity Ratios<br><br>\n",
    "Current Ratio: Measures a company’s ability to meet short-term obligations.<br>\n",
    "Quick Ratio (Acid Test): Similar to the current ratio but excludes inventory.<br>\n",
    "Cash Ratio: Measures a company's ability to meet short-term obligations with cash and cash equivalents.<br>\n",
    "Solvency Ratios<br><br>\n",
    "Debt-to-Equity Ratio: Measures the proportion of debt to equity financing.<br>\n",
    "Interest Coverage Ratio: Measures a company's ability to meet interest payments on its debt.<br>\n",
    "Debt-to-Asset Ratio: Measures the proportion of assets financed by debt.<br>\n",
    "##### Efficiency Ratios<br><br>\n",
    "Inventory Turnover Ratio: Measures how efficiently a company manages its inventory.<br>\n",
    "Days Sales Outstanding (DSO): Measures the average number of days it takes to collect receivables.<br>\n",
    "Accounts Payable Turnover: Measures how efficiently a company manages its payables.<br>\n",
    "Asset Turnover Ratio: Measures how efficiently a company generates sales from its assets.<br>\n",
    "##### Valuation Ratios<br><br>\n",
    "Price-to-Earnings (P/E) Ratio: Measures the price investors are willing to pay for each dollar of earnings.<br>\n",
    "Price-to-Book (P/B) Ratio: Compares a company's market value to its book value.<br>\n",
    "Price-to-Sales (P/S) Ratio: Measures the price investors are willing to pay for each dollar of sales.<br>\n",
    "Dividend Yield: Measures the annual dividend per share relative to the share price.<br>\n",
    "Dividend Payout Ratio: Measures the proportion of earnings paid out as dividends.<br>\n",
    "Price-to-Cash Flow (P/CF) Ratio: Compares a company's market value to its operating cash flow.<br>\n",
    "Free Cash Flow Yield: Measures the free cash flow generated per share relative to the share price.<br>\n",
    "##### Growth Ratios<br><br>\n",
    "Earnings Per Share (EPS) Growth Rate: Measures the growth rate of earnings per share.<br>\n",
    "Revenue Growth Rate: Measures the growth rate of a company's sales.<br>\n",
    "Book Value Per Share Growth Rate: Measures the growth rate of a company's book value per share.<br>\n",
    "##### Additional Ratios<br><br>\n",
    "PEG Ratio: Compares the P/E ratio to the expected earnings growth rate.<br>\n",
    "Enterprise Value (EV) to EBITDA: Compares a company's enterprise value to its earnings before interest, taxes, depreciation, and amortization.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f60095b-7f19-4daa-a639-051cd69a6055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of selected features based on the importance for stock analysis\n",
    "def plot_full_correlation_heatmap(df, features_of_interest, year, title):\n",
    "    \"\"\" Plot a correlation heatmap for all specified features in a given year \"\"\"\n",
    "    # Filter data for the specific year\n",
    "    df_year = df[df['Year'] == year]\n",
    "\n",
    "    # Calculate correlation matrix for the selected columns\n",
    "    correlation_matrix = df_year[features_of_interest].corr()\n",
    "\n",
    "    # Plotting the heatmap\n",
    "    plt.figure(figsize=(14, 10))  # Set figure size\n",
    "    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=90)  # Rotate feature names for better readability\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "selected_features = [\n",
    "    'grossProfitMargin',\n",
    "    'operatingProfitMargin',\n",
    "    'netProfitMargin',\n",
    "    'returnOnEquity',\n",
    "    'returnOnAssets',\n",
    "    'returnOnCapitalEmployed',\n",
    "    'currentRatio',\n",
    "    'quickRatio',\n",
    "    'cashRatio',\n",
    "    'debtEquityRatio',\n",
    "    'interestCoverage',\n",
    "    'debtRatio',\n",
    "    'inventoryTurnover',\n",
    "    'daysOfSalesOutstanding',\n",
    "    'payablesTurnover',\n",
    "    'assetTurnover',\n",
    "    'PE ratio',\n",
    "    'PB ratio',\n",
    "    'priceSalesRatio',\n",
    "    'dividendYield',\n",
    "    'dividendPayoutRatio',\n",
    "    'priceCashFlowRatio',\n",
    "    'Free Cash Flow Yield',  \n",
    "    'EPS Growth',\n",
    "    'Revenue Growth',\n",
    "    'Book Value per Share Growth',\n",
    "    'priceEarningsToGrowthRatio',\n",
    "    'Enterprise Value over EBITDA',\n",
    "    'Year'\n",
    "]\n",
    "value_features_df = full_df[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74cec7d-de4d-4307-bbb7-3319b0c33103",
   "metadata": {},
   "source": [
    "### Plot the features for year 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26909b4f-ac5b-4297-9e02-15c83adccdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_year(value_features_df, 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac310ab-e07c-4a25-86c5-0a17b38af6e2",
   "metadata": {},
   "source": [
    "### Now let's create a heatmap to see how this features are correlated with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d324d0ab-6cb6-47c2-83a5-31cfd65aebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2014\n",
    "plot_full_correlation_heatmap(full_df, selected_features, 2014, \"Full Correlation Heatmap for the Year 2014\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a5f3ba-1feb-4d22-9ae0-8f5cc9c4cd9e",
   "metadata": {},
   "source": [
    "# Observations:\n",
    "\n",
    "## High Positive Correlations:\n",
    "\n",
    "1. **Profit Margins and Returns:**\n",
    "   - `grossProfitMargin`, `operatingProfitMargin`, and `netProfitMargin` show strong positive correlations with `returnOnEquity`, `returnOnAssets`, and `returnOnCapitalEmployed`.\n",
    "   - This suggests that higher profit margins are generally associated with higher returns on equity, assets, and capital employed.\n",
    "\n",
    "2. **Liquidity Ratios:**\n",
    "   - `currentRatio`, `quickRatio`, and `cashRatio` show positive correlations with each other.\n",
    "   - This indicates that they tend to move in the same direction as expected since they measure the company's ability to cover its short-term obligations.\n",
    "\n",
    "## Negative Correlations:\n",
    "\n",
    "1. **Debt Ratios vs. Returns:**\n",
    "   - `debtEquityRatio` and `debtRatio` show negative correlations with `returnOnEquity` and `returnOnAssets`.\n",
    "   - This implies that companies with higher debt tend to have lower returns on equity and assets.\n",
    "\n",
    "2. **Profitability vs. Growth Metrics:**\n",
    "   - Some growth metrics like `Revenue Growth` and `EPS Growth` do not necessarily correlate strongly with current profitability ratios.\n",
    "   - This indicates that high current profits do not automatically translate to high growth rates.\n",
    "\n",
    "## Weak or Insignificant Correlations:\n",
    "\n",
    "1. **Valuation and Profitability:**\n",
    "   - Metrics like `PE ratio`, `PB ratio`, and valuation ratios show relatively weak correlations with profitability and liquidity metrics.\n",
    "   - This suggests that market valuation ratios do not always reflect the current financial operational performance directly.\n",
    "\n",
    "2. **Dividend Metrics:**\n",
    "   - `dividendYield` and `dividendPayoutRatio` do not show strong correlations with many of the profitability or liquidity metrics.\n",
    "   - This indicates that dividend policies may be more influenced by other factors like management decisions or historical payout ratios rather than current financial health.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee6a245-f4a3-4721-b1c6-0c2d7ef104ef",
   "metadata": {},
   "source": [
    "## Check missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761feaa0-6f0a-4603-8cbe-45361a95339f",
   "metadata": {},
   "source": [
    "Let's create a new Dataframe to use the original dataset in case we need it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6bca5b-7739-4003-b410-192a679fc16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d6701c-71c9-4acf-9df2-948328c044a6",
   "metadata": {},
   "source": [
    "let's check null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5259bfa5-aa1c-430a-8d42-752e99a0d0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_missing_values = full_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eadb55-c3c0-497a-9113-934a52e98cf0",
   "metadata": {},
   "source": [
    "let's see the percentage of missing values per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a44aea-463f-47d7-9048-98491949ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column, value in initial_missing_values.items():\n",
    "    percentage = round((value / len(full_df)) * 100, 2)\n",
    "    print(f\"{column}: {value} ({percentage}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d48386-14ec-4abb-a7c0-c1fd15f4e8d2",
   "metadata": {},
   "source": [
    "## Calculate the percentage of missing data by percentage of missingness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b02baa-69a0-4a14-9be9-7ca3bf7d626f",
   "metadata": {},
   "source": [
    "### Missing < 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7b52b-cf28-4b8f-abe2-d10621d5f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing data by percentage of missingness\n",
    "def percentage_of_nan_classification(min,max,df):\n",
    "    for column, value in initial_missing_values.items():\n",
    "        percentage = round((value / len(df)) * 100, 2)\n",
    "        if percentage >= min and percentage <= max:\n",
    "            print(f\"{column}: {value} ({percentage}%)\")\n",
    "percentage_of_nan_classification(0,10,full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7f342-85b2-49ae-bf2e-6dd877a0e04e",
   "metadata": {},
   "source": [
    "### Missing > 10 & Missing <= 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcdc26e-7564-4741-b3ee-38e138bcebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_of_nan_classification(10,20,full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0c7d5-22aa-4c9c-8fd8-b3f86ba6848f",
   "metadata": {},
   "source": [
    "### Missing > 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30248aed-9a13-4089-9917-95ed146092af",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_of_nan_classification(20,100,full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a5a10-37ee-47e6-aaaf-7cdb3763d886",
   "metadata": {},
   "source": [
    "# Let's use Median and KNN to impute the data and see if the underlying distributions does not change much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee76bceb-b010-461a-9bfe-6a6e84ff7325",
   "metadata": {},
   "source": [
    "# Data Imputation and Distribution Analysis\n",
    "\n",
    "This set of functions is designed to facilitate data imputation for datasets with missing values and to assess the impact of imputation on the statistical distribution of each feature within the datasets. The analysis primarily uses the Kolmogorov-Smirnov (KS) test to determine if imputation has significantly altered the distributions, aiding in deciding whether the imputed data is suitable for further analysis or needs additional refinement.\n",
    "\n",
    "## Function Descriptions and Workflow\n",
    "\n",
    "### `apply_imputation(data, imputer)`\n",
    "\n",
    "**Purpose**: Applies specified imputation technique to numeric columns of a dataset and retains non-numeric data unchanged.\n",
    "\n",
    "**Input**:\n",
    "- `data`: DataFrame containing the dataset with missing values.\n",
    "- `imputer`: An instance of `SimpleImputer` or `KNNImputer` from scikit-learn configured with the desired imputation strategy.\n",
    "\n",
    "**Process**:\n",
    "- Separates numeric and non-numeric data.\n",
    "- Applies the imputation to only numeric data.\n",
    "- Reintegrates imputed numeric data with the original non-numeric data.\n",
    "\n",
    "**Output**: Returns the imputed dataset along with descriptive statistics (before and after imputation) for each numeric feature.\n",
    "\n",
    "**Usage**: This function is typically used within a data preprocessing pipeline where handling of missing values is necessary.\n",
    "\n",
    "### `visualize_and_compare(data_before, data_after, feature, ks_threshold, p_value_threshold)`\n",
    "\n",
    "**Purpose**: Visualizes the distribution of a feature before and after imputation and performs the KS test to evaluate changes.\n",
    "\n",
    "**Input**:\n",
    "- `data_before`: DataFrame containing the feature values before imputation.\n",
    "- `data_after`: DataFrame containing the feature values after imputation.\n",
    "- `feature`: The name of the feature to analyze.\n",
    "- `ks_threshold`: KS statistic threshold to decide significant change.\n",
    "- `p_value_threshold`: p-value threshold to decide significant change.\n",
    "\n",
    "**Process**:\n",
    "- Generates histogram and boxplot for visual comparison.\n",
    "- Calculates the KS statistic and p-value between the distributions before and after imputation.\n",
    "\n",
    "**Output**: Returns a boolean indicating if the feature needs review and a text explanation of the KS test results.\n",
    "\n",
    "**Usage**: Helps in visualizing how individual features are affected by imputation and whether their distribution changes are within acceptable limits.\n",
    "\n",
    "### `analyze_ks_statistics(data_before, data_after, ks_threshold, p_value_threshold)`\n",
    "\n",
    "**Purpose**: Applies the KS test across all numeric features to identify those that have significantly changed post-imputation.\n",
    "\n",
    "**Input**:\n",
    "- `data_before`, `data_after`: DataFrames of the dataset before and after imputation.\n",
    "- `ks_threshold`, `p_value_threshold`: Thresholds for the KS statistic and p-value to flag significant changes.\n",
    "\n",
    "**Process**:\n",
    "- Iterates through each numeric feature, applying `visualize_and_compare`.\n",
    "\n",
    "**Output**: Returns a dictionary listing features that need further review based on the KS test results.\n",
    "\n",
    "**Usage**: Integral for automating the review process of features after imputation, ensuring data integrity.\n",
    "\n",
    "### `impute_and_analyze(datasets, imputer_type, ks_threshold, p_value_threshold)`\n",
    "\n",
    "**Purpose**: Coordinates the entire process of imputation and post-imputation analysis for multiple datasets.\n",
    "\n",
    "**Input**:\n",
    "- `datasets`: Dictionary of DataFrames keyed by identifiers (e.g., year or dataset name).\n",
    "- `imputer_type`: Choice between 'median' or 'knn' for the type of imputation.\n",
    "- `ks_threshold`, `p_value_threshold`: Criteria for significant change post-imputation.\n",
    "\n",
    "**Process**:\n",
    "- For each dataset, performs imputation, applies KS analysis, and aggregates results.\n",
    "\n",
    "**Output**: Returns detailed results of the imputation and a list of features that require review across all datasets.\n",
    "\n",
    "**Usage**: Suitable for batch processing of multiple datasets, providing a comprehensive overview of imputation effects and ensuring data quality for subsequent analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e9763b-b621-4d9e-a0ac-8f5ebea37311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def apply_imputation(data, imputer):\n",
    "    \"\"\"Applies imputation and returns the imputed dataset along with before and after statistics.\"\"\"\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    non_numeric_data = data.select_dtypes(exclude=[np.number])\n",
    "    stats_before = numeric_data.describe().transpose()\n",
    "    imputed_data = imputer.fit_transform(numeric_data)\n",
    "    imputed_df = pd.DataFrame(imputed_data, columns=numeric_data.columns)\n",
    "    imputed_df = pd.concat([imputed_df, non_numeric_data], axis=1)\n",
    "    stats_after = imputed_df.describe().transpose()\n",
    "    return imputed_df, stats_before, stats_after\n",
    "\n",
    "def visualize_and_compare(data_before, data_after, feature, ks_threshold=0.10, p_value_threshold=0.05):\n",
    "    \"\"\"Creates histograms and boxplots for the given feature before and after imputation, evaluates changes.\"\"\"\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    bins = np.linspace(min(data_before[feature].min(), data_after[feature].min()), max(data_before[feature].max(), data_after[feature].max()), 30)\n",
    "    data_before[feature].hist(alpha=0.5, bins=bins, label='Before', density=True)\n",
    "    data_after[feature].hist(alpha=0.5, bins=bins, label='After', density=True)\n",
    "    plt.legend()\n",
    "    plt.title(f'Histogram of {feature}')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot([data_before[feature].dropna(), data_after[feature]], labels=['Before', 'After'], showfliers=True)\n",
    "    plt.title(f'Boxplot of {feature}')\n",
    "    plt.show()\n",
    "\n",
    "    ks_stat, ks_pvalue = ks_2samp(data_before[feature].dropna(), data_after[feature])\n",
    "    needs_review = ks_stat > ks_threshold and ks_pvalue < p_value_threshold\n",
    "    explanation = (f\"KS Test for {feature}: Statistic={ks_stat:.4f}, P-value={ks_pvalue:.4f}.\\n\"\n",
    "                   f\"{'Significant changes detected, review needed.' if needs_review else 'Changes are minimal, imputation appears effective.'}\")\n",
    "    print(explanation)\n",
    "    return needs_review, explanation\n",
    "\n",
    "def analyze_ks_statistics(data_before, data_after, ks_threshold=0.10, p_value_threshold=0.05):\n",
    "    \"\"\"Analyzes KS statistics for all features, identifies which need further review.\"\"\"\n",
    "    review_list = []\n",
    "    for feature in data_before.select_dtypes(include=[np.number]).columns:\n",
    "        needs_review, _ = visualize_and_compare(data_before, data_after, feature, ks_threshold, p_value_threshold)\n",
    "        if needs_review:\n",
    "            review_list.append(feature)\n",
    "    return review_list\n",
    "\n",
    "def impute_and_analyze(datasets, imputer_type='median', ks_threshold=0.10, p_value_threshold=0.05):\n",
    "    results = {}\n",
    "    review_lists = {}\n",
    "    imputed_datasets = {}  # To store imputed datasets for each year\n",
    "    for year, data in datasets.items():\n",
    "        # Choose the imputer based on the specified type\n",
    "        imputer = SimpleImputer(strategy=imputer_type) if imputer_type == 'median' else KNNImputer(n_neighbors=5)\n",
    "        imputed_data, stats_before, stats_after = apply_imputation(data, imputer)\n",
    "        results[year] = imputed_data\n",
    "        imputed_datasets[year] = imputed_data  # Store the imputed dataset\n",
    "        \n",
    "        # Analyze using the corrected approach\n",
    "        review_lists[year] = analyze_ks_statistics(data, imputed_data, ks_threshold, p_value_threshold)\n",
    "        \n",
    "        print(f\"Imputation completed for {year}. Features to review due to significant changes: {len(review_lists[year])}\")\n",
    "        if review_lists[year]:\n",
    "            print(\"Review required for the following features:\")\n",
    "            for feature in review_lists[year]:\n",
    "                print(f\"{feature}\")\n",
    "                \n",
    "    return results, review_lists, imputed_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5fb0c-bffd-4698-98f5-adbd421f1106",
   "metadata": {},
   "source": [
    "#### Let's use median and see the results after imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f6fa96-12a1-4246-be9d-e7c5cad8b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets call it\n",
    "datasets = {\n",
    "    '2014': data_2014,\n",
    "    '2015': data_2015,\n",
    "    '2016': data_2016,\n",
    "    '2017': data_2017,\n",
    "    '2018': data_2018,\n",
    "    #'Full': full_df\n",
    "}\n",
    "df_results_median, review_list, df_imputed_median = impute_and_analyze(datasets, imputer_type='median')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c968e5ca-bb65-4816-8cc0-41b7559601aa",
   "metadata": {},
   "source": [
    "#### Let's use KNN and see the results after imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f4b45-33ed-4814-82ad-b15ced732cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_knn, review_list_knn, df_imputed_knn = impute_and_analyze(datasets, imputer_type='knn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8899b0-17f6-4576-908b-43494665bf70",
   "metadata": {},
   "source": [
    "# Comparison of Median and KNN Imputation Methods\n",
    "\n",
    "In our analysis, we compared the effectiveness of median and KNN imputation methods by examining the number of features that showed significant changes after imputation. The Kolmogorov-Smirnov (KS) test was used to identify features with significant distribution changes that require further review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d7d650-9a06-4f7d-8fb4-9f2b2a275616",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b05a1-8f8d-47ce-a0c8-d79d729a1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Features that need review using median imputation--> 2014: {len(review_list['2014'])}, 2015: {len(review_list['2015'])}, 2016: {len(review_list['2016'])}, 2017: {len(review_list['2017'])}, 2018: {len(review_list['2018'])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed326199-4dc2-4fbe-b014-ff8a9c1f56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87407ca-a51d-4485-8c4f-c943cf20771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Features that need review--> 2014: {len(review_list_knn['2014'])}, 2015: {len(review_list_knn['2015'])}, 2016: {len(review_list_knn['2016'])}, 2017: {len(review_list_knn['2017'])}, 2018: {len(review_list_knn['2018'])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68d1c2c-edce-4039-a8d6-caeb66cfa495",
   "metadata": {},
   "source": [
    "## Features Needing Review\n",
    "\n",
    "### Median Imputation\n",
    "The number of features needing review for each year using median imputation are as follows:\n",
    "- **2014**: 47 features\n",
    "- **2015**: 37 features\n",
    "- **2016**: 53 features\n",
    "- **2017**: 59 features\n",
    "- **2018**: 27 features\n",
    "\n",
    "### KNN Imputation\n",
    "The number of features needing review for each year using KNN imputation are as follows:\n",
    "- **2014**: 8 features\n",
    "- **2015**: 4 features\n",
    "- **2016**: 13 features\n",
    "- **2017**: 9 features\n",
    "- **2018**: 6 features\n",
    "\n",
    "## Conclusion\n",
    "The KNN imputation method resulted in fewer features needing review due to significant distribution changes compared to the median imputation method. This suggests that KNN imputation may better preserve the original data distributions in this context.\n",
    "\n",
    "It is important to note also that if we compare the features that had more than 20% o missing values and the features that need to be reviewed are the same, this makes sense because the underlying distribution can change dramatically becase of the predictions of the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fd619-fce0-45f4-b294-9a6b97438d20",
   "metadata": {},
   "source": [
    "### Now let's check out new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566126ff-c899-4566-89ab-e98d24032e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the imputed data for a specific year (e.g., 2014)\n",
    "imputed_data_2014 = df_imputed_knn['2014']\n",
    "imputed_data_2015 = df_imputed_knn['2015']\n",
    "imputed_data_2016 = df_imputed_knn['2016']\n",
    "imputed_data_2017 = df_imputed_knn['2017']\n",
    "imputed_data_2018 = df_imputed_knn['2018']\n",
    "\n",
    "# Display the first few rows of the imputed DataFrame\n",
    "imputed_data_2014.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74117f61-0c03-4e1f-a925-28948877e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features_to_review(imputed_datasets, review_lists):\n",
    "    \"\"\"Drops specified features from the imputed datasets.\"\"\"\n",
    "    # Combine all features that need review into a single set\n",
    "    features_to_drop = set()\n",
    "    for features in review_lists.values():\n",
    "        features_to_drop.update(features)\n",
    "    \n",
    "    # Drop the features from the imputed datasets\n",
    "    cleaned_datasets = {}\n",
    "    for year, data in imputed_datasets.items():\n",
    "        cleaned_data = data.drop(columns=features_to_drop, errors='ignore')\n",
    "        cleaned_datasets[year] = cleaned_data\n",
    "    \n",
    "    return cleaned_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e8318-4294-44b4-8343-58ef1d3b95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_dfs = {\n",
    "    '2014': imputed_data_2014,\n",
    "    '2015': imputed_data_2015,\n",
    "    '2016': imputed_data_2016,\n",
    "    '2017': imputed_data_2017,\n",
    "    '2018': imputed_data_2018,\n",
    "}\n",
    "cleaned_datasets = drop_features_to_review(imputed_dfs, review_list_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b88e85-c56f-4a48-938f-618661e531cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape for 2014: {cleaned_datasets['2014'].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee507c-2aa2-41d3-92ef-23443c7889b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape for 2015: {cleaned_datasets['2015'].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a195981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape for 2016: {cleaned_datasets['2016'].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape for 2017: {cleaned_datasets['2017'].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688316fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape for 2018: {cleaned_datasets['2018'].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac5aa75-accf-43e2-b0f8-2c8f5128b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas_datareader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9032d95-25fd-454c-9ac9-6f2d290f52f1",
   "metadata": {},
   "source": [
    "# Outliers\n",
    "\n",
    "From the boxplots we generated previously, we observed that there is at least one significant outlier in every feature. Identifying and handling these outliers is crucial for ensuring the quality and accuracy of our data analysis. Let's investigate these outliers and develop a strategy to address them.\n",
    "\n",
    "We'll focus on identifying stocks with unusual price variations, verifying the organic nature of their gains, and then cleaning the dataset accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ca019-3c54-4ae7-8f91-3db1e895e665",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "#### 1. Extract Target Data\n",
    "This step focuses on isolating the `PRICE VAR [%]` column and the `Sector` column for the specified year. By doing this, we can specifically analyze the price variation data for each stock in that year.\n",
    "\n",
    "#### 2. Plot Sector Data\n",
    "To identify potential outliers, we visualize the price variations for each sector. This helps us spot major peaks or valleys that might indicate unusual stock performance.\n",
    "\n",
    "#### 3. Investigate Top Gainers\n",
    "For stocks that show significant gains, we fetch their daily prices and trading volumes. By plotting these, we can verify if the growth was organic or if there were periods of no trading activity, which might indicate inorganic growth.\n",
    "\n",
    "#### 4. Clean Dataset\n",
    "After identifying stocks with inorganic gains, we remove them from the dataset. This ensures that our analysis and predictions are based on genuinely performing stocks, which is crucial for accurately predicting and potentially outperforming the S&P 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cafe5b7-d266-48c8-8755-98b2f1f021df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def clean_dataset_by_year(df, year, price_var_year, gain_threshold=500):\n",
    "    \"\"\"Cleans the dataset for a specific year by removing stocks with inorganic gains.\"\"\"\n",
    "    \n",
    "    # Extract the price variation column for the given year\n",
    "    price_var_col = f'{price_var_year} PRICE VAR [%]'\n",
    "    df_ = df.loc[:, ['Sector', price_var_col, 'Unnamed: 0']]  # Assuming 'Unnamed: 0' is the company symbol\n",
    "    \n",
    "    # Get list of sectors\n",
    "    sector_list = df_['Sector'].unique()\n",
    "    \n",
    "    # Plot the percent price variation for each sector\n",
    "    for sector in sector_list:\n",
    "        temp = df_[df_['Sector'] == sector]\n",
    "        plt.figure(figsize=(30,5))\n",
    "        plt.plot(temp[price_var_col])\n",
    "        plt.title(sector.upper(), fontsize=20)\n",
    "        plt.show()\n",
    "    \n",
    "    # Identify stocks that gained more than the threshold\n",
    "    top_gainers = df_[df_[price_var_col] >= gain_threshold]\n",
    "    top_gainers = top_gainers.sort_values(by=price_var_col, ascending=False)\n",
    "    \n",
    "    # Set date range for the given year\n",
    "    date_start = f'{price_var_year}-01-01'\n",
    "    date_end = f'{price_var_year}-12-31'\n",
    "    tickers = top_gainers['Unnamed: 0'].values.tolist()\n",
    "    \n",
    "    # Verify organic growth of top gainers\n",
    "    inorganic_stocks = []\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            daily_price = pdr.DataReader(ticker, 'yahoo', date_start, date_end)\n",
    "            fig, (ax0, ax1) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [3, 1]})\n",
    "            ax0.plot(daily_price['Adj Close'])\n",
    "            ax0.set_title(ticker, fontsize=18)\n",
    "            ax0.set_ylabel('Daily Adj Close $', fontsize=14)\n",
    "            ax1.plot(daily_price['Volume'])\n",
    "            ax1.set_ylabel('Volume', fontsize=14)\n",
    "            ax1.yaxis.set_major_formatter(ticker.StrMethodFormatter('{x:.0E}'))\n",
    "            fig.align_ylabels(ax1)\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Check for non-organic growth (flat portions in price trend)\n",
    "            if daily_price['Adj Close'].diff().eq(0).all():\n",
    "                inorganic_stocks.append(ticker)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {ticker}: {e}\")\n",
    "            inorganic_stocks.append(ticker)\n",
    "    \n",
    "    # Remove inorganic gainers from the dataframe\n",
    "    df = df[~df['Unnamed: 0'].isin(inorganic_stocks)]\n",
    "    \n",
    "    return df, inorganic_stocks\n",
    "\n",
    "# map dictionary containing the datasets\n",
    "cleaned_datasets_updated = {}\n",
    "price_var_years = {\n",
    "    '2014': '2015',\n",
    "    '2015': '2016',\n",
    "    '2016': '2017',\n",
    "    '2017': '2018',\n",
    "    '2018': '2019'  \n",
    "}\n",
    "\n",
    "for year, df in cleaned_datasets.items():\n",
    "    cleaned_datasets_updated[year], istocks = clean_dataset_by_year(df, year, price_var_years[year])\n",
    "\n",
    "# accessing the cleaned data for a specific year with outliers handled\n",
    "cleaned_data_2014 = cleaned_datasets_updated['2014']\n",
    "cleaned_data_2015 = cleaned_datasets_updated['2015']\n",
    "cleaned_data_2016 = cleaned_datasets_updated['2016']\n",
    "cleaned_data_2017 = cleaned_datasets_updated['2017']\n",
    "cleaned_data_2018 = cleaned_datasets_updated['2018']\n",
    "\n",
    "cleaned_data_2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45920e83-aaa6-4b45-98fc-37ff75352b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape for 2014 before: {imputed_dfs['2014'].shape}')\n",
    "print(f'Shape for 2014 after: {cleaned_data_2014.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2fbc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape for 2015 before: {imputed_dfs['2015'].shape}')\n",
    "print(f'Shape for 2015 after: {cleaned_data_2015.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape for 2016 before: {imputed_dfs['2016'].shape}')\n",
    "print(f'Shape for 2016 after: {cleaned_data_2016.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7724ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape for 2017 before: {imputed_dfs['2017'].shape}')\n",
    "print(f'Shape for 2017 after: {cleaned_data_2017.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbec5f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape for 2018 before: {imputed_dfs['2018'].shape}')\n",
    "print(f'Shape for 2018 after: {cleaned_data_2018.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271f9920-207a-4084-8dc8-ddb2ee24a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "istocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e803cc3a-bcd0-4e39-acb8-c0c1e12f3259",
   "metadata": {},
   "source": [
    "### Outlier Management in Data Analysis\n",
    "\n",
    "During our comprehensive review of the annual financial performance data, we have identified specific instances where stocks exhibited extreme price variations that deviate significantly from typical sector trends. These anomalies, or 'outliers,' could potentially skew our analysis and lead to less accurate forecasts.\n",
    "\n",
    "#### Decision to Remove Outliers\n",
    "\n",
    "To enhance the robustness and reliability of our predictive models, we have decided to remove these outliers from our dataset. This step ensures that our analysis is grounded in data that accurately reflects the majority of market behaviors without being disproportionately influenced by extreme values.\n",
    "\n",
    "By focusing on more consistent data, we aim to provide more reliable insights and strategies for outperforming the S&P 500. This approach is crucial for maintaining the integrity of our financial models and supporting strategic investment decisions based on sound statistical principles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6654597f-b6cc-417d-9927-33bd3ae56006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_quantile_outliers(cleaned_datasets):\n",
    "    \"\"\"\n",
    "    Adjusts the datasets by capping and flooring the values at the 97th and 3rd percentiles respectively, handling numeric data only.\n",
    "\n",
    "    Parameters:\n",
    "    cleaned_datasets (dict): A dictionary where keys are years and values are DataFrames of the datasets for those years.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary of DataFrames with outliers adjusted.\n",
    "    \"\"\"\n",
    "    cleaned_datasets_updated = {}\n",
    "    \n",
    "    for year, df in cleaned_datasets.items():\n",
    "        # Separate numeric and non-numeric data\n",
    "        numeric_data = df.select_dtypes(include=[np.number])\n",
    "        non_numeric_data = df.select_dtypes(exclude=[np.number])\n",
    "\n",
    "        # Compute quantiles and identify outliers\n",
    "        top_quantiles = numeric_data.quantile(0.97)\n",
    "        outliers_top = (numeric_data > top_quantiles)\n",
    "\n",
    "        low_quantiles = numeric_data.quantile(0.03)\n",
    "        outliers_low = (numeric_data < low_quantiles)\n",
    "\n",
    "        # Apply masks to cap and floor outlier values\n",
    "        numeric_data = numeric_data.mask(outliers_top, top_quantiles, axis=1)\n",
    "        numeric_data = numeric_data.mask(outliers_low, low_quantiles, axis=1)\n",
    "        \n",
    "        # Concatenate the numeric and non-numeric data back together\n",
    "        cleaned_data = pd.concat([numeric_data, non_numeric_data], axis=1)\n",
    "        cleaned_datasets_updated[year] = cleaned_data\n",
    "\n",
    "        # Optionally print or log the description of the cleaned dataset\n",
    "        print(f\"Data for {year} after outlier adjustment:\")\n",
    "        print(cleaned_data.describe())\n",
    "\n",
    "    return cleaned_datasets_updated\n",
    "\n",
    "# Example usage with your cleaned datasets\n",
    "cleaned_datasets_with_adjusted_outliers = remove_quantile_outliers(cleaned_datasets_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb891b8-e8fa-4bc2-93ab-660c3ffcd0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_datasets_with_adjusted_outliers['2014']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12bee9-221e-4ac0-8065-c9c056b01f95",
   "metadata": {},
   "source": [
    "## Now Let's plot again to see ditributions and outliers after cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735af25-bdfd-43fc-883c-0823695165b4",
   "metadata": {},
   "source": [
    "## 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9731e9c-b959-4115-9a9b-7ab8d072b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_year(cleaned_datasets_with_adjusted_outliers['2014'], 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90efcfe4-d8b1-4d62-9c42-e618f0572835",
   "metadata": {},
   "source": [
    "## 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb6a36b-c1b8-4072-b975-d401cec88a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_year(cleaned_datasets_with_adjusted_outliers['2015'], 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028186bc-643f-486f-91b1-2fb8ce81d220",
   "metadata": {},
   "source": [
    "## 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be7460-b89b-459e-bfad-c29b1d42829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_year(cleaned_datasets_with_adjusted_outliers['2016'], 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61242d-45fd-4814-8625-16d5ce946aaf",
   "metadata": {},
   "source": [
    "## 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fed36a5-3981-4009-b58d-b110cb9b1b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_year(cleaned_datasets_with_adjusted_outliers['2017'], 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd227293-ab8a-4e8d-8f68-d00d491246bd",
   "metadata": {},
   "source": [
    "## 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2dafaf-cae7-4a49-ba22-b9f7aa156cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_year(cleaned_datasets_with_adjusted_outliers['2018'], 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe6ea32-3a5f-4bce-86b1-843775b2c538",
   "metadata": {},
   "source": [
    "### We still have outliers but looks much better. Let's save the cleaned new datasets to start with feature engieniering and modeling in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651feac8-86db-47c1-8f6e-a1387af09639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned_datasets(cleaned_datasets_updated, directory=\"processed_data\"):\n",
    "    \"\"\"\n",
    "    Saves the cleaned datasets to the specified directory with names corresponding to their respective years.\n",
    "    \n",
    "    Parameters:\n",
    "    cleaned_datasets_updated (dict): A dictionary where keys are years and values are DataFrames of the cleaned datasets for those years.\n",
    "    directory (str): Path where files will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Iterate through the dictionary to save each DataFrame\n",
    "    for year, df in cleaned_datasets_updated.items():\n",
    "        # Rename the 'Unnamed: 0' column to 'Symbol'\n",
    "        if 'Unnamed: 0' in df.columns:\n",
    "            df = df.rename(columns={'Unnamed: 0': 'Symbol'})\n",
    "\n",
    "        # Define the filename\n",
    "        filename = f\"clean_df_{year}.csv\"\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        # Save the DataFrame\n",
    "        df.to_csv(file_path, index=False)\n",
    "\n",
    "        # Optionally print a confirmation\n",
    "        print(f\"Saved cleaned dataset for {year} to {file_path}\")\n",
    "\n",
    "# Example usage assuming cleaned_datasets_updated is defined\n",
    "save_cleaned_datasets(cleaned_datasets_with_adjusted_outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f912fb-8da3-4562-a781-1aba9dc1edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('processed_data/clean_df_2014.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
