{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de0e3e-2eef-4597-94da-f6d55532bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc355a95-d71b-494c-8b08-584bf14143f6",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "### Objective\n",
    "The goal of this notebook is to perform PCA on financial data from 2014 to 2018 to reduce the number of features and identify the principal components that capture the most variance in the data.\n",
    "\n",
    "### Methodology\n",
    "1. **Data Loading**: Load the processed data for each year.\n",
    "2. **Standardization**: Standardize the features since PCA is affected by scale.\n",
    "3. **PCA Execution**: Perform PCA to reduce dimensions while retaining 95% of the variance.\n",
    "4. **Results Analysis**: Plot and examine the explained variance to determine how many features are necessary to capture the majority of the information.\n",
    "\n",
    "### Results\n",
    "The PCA results will help in understanding the underlying structure of the data and guide further analyses and modeling efforts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7236a87-43d5-474d-a8fa-e15a6abc3014",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2014 = pd.read_csv('processed_data/clean_df_2014.csv')\n",
    "data_2015 = pd.read_csv('processed_data/clean_df_2015.csv')\n",
    "data_2016 = pd.read_csv('processed_data/clean_df_2016.csv')\n",
    "data_2017 = pd.read_csv('processed_data/clean_df_2017.csv')\n",
    "data_2018 = pd.read_csv('processed_data/clean_df_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf2fc92-336c-449b-9ac3-59d7d4852a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pca_analysis(data, num_top_features=10):\n",
    "    \"\"\"\n",
    "    Perform PCA on the provided dataset and visualize the explained variance.\n",
    "    Also, provide the loadings for the top features of each component.\n",
    "    \n",
    "    Parameters:\n",
    "    - data (DataFrame): The dataset to perform PCA on.\n",
    "    - num_top_features (int): Number of top features' loadings to return for each component.\n",
    "    \n",
    "    Returns:\n",
    "    - pca: The PCA model.\n",
    "    - explained_variance_plot: Matplotlib figure object for the explained variance plot.\n",
    "    - loadings_dict: Dictionary containing the loadings for the top features of each component.\n",
    "    \"\"\"\n",
    "    # Isolate numeric data for PCA\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(numeric_data)\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_data)\n",
    "    \n",
    "    # Explained variance plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Explained Variance by PCA Components')\n",
    "    plt.grid(True)\n",
    "    explained_variance_plot = plt\n",
    "    \n",
    "    # Create a dictionary to store loadings for each component\n",
    "    loadings_dict = {}\n",
    "    for i in range(len(pca.components_)):\n",
    "        loadings = pca.components_[i]\n",
    "        loading_scores = pd.Series(loadings, index=numeric_data.columns)\n",
    "        sorted_loading_scores = loading_scores.abs().sort_values(ascending=False)\n",
    "        loadings_dict[f\"Component {i+1}\"] = sorted_loading_scores.head(num_top_features)\n",
    "    \n",
    "    return pca, explained_variance_plot, loadings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b76328-6b7c-4b34-bab5-89ec896cbc95",
   "metadata": {},
   "source": [
    "# 2014 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d44f47-1c67-4f36-bcc1-5c46f351c0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40b05b7-5fc1-4531-80e7-2c4934ebfbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2014 dataset\n",
    "pca_model, ev_plot, loadings_2014 = perform_pca_analysis(data_2014)\n",
    "ev_plot.show()\n",
    "\n",
    "# Displaying loadings\n",
    "print(' --- COMPONENT 1 --- ')\n",
    "print(loadings_2014[\"Component 1\"])\n",
    "print(' --- COMPONENT 2 --- ')\n",
    "print(loadings_2014[\"Component 2\"])\n",
    "print(' --- COMPONENT 3 --- ')\n",
    "print(loadings_2014[\"Component 3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a4fe2b-4750-4909-a61d-a2dbddccf681",
   "metadata": {},
   "source": [
    "# 2015 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec5131-0ca2-4565-a669-e2cae868faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2015 dataset\n",
    "pca_model, ev_plot, loadings_2015 = perform_pca_analysis(data_2015)\n",
    "ev_plot.show()\n",
    "\n",
    "# Displaying loadings\n",
    "print(' --- COMPONENT 1 --- ')\n",
    "print(loadings_2014[\"Component 1\"])\n",
    "print(' --- COMPONENT 2 --- ')\n",
    "print(loadings_2014[\"Component 2\"])\n",
    "print(' --- COMPONENT 3 --- ')\n",
    "print(loadings_2014[\"Component 3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3093c60a-c590-4069-99f8-bd72ebf52c8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2016 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc88b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016 dataset\n",
    "pca_model, ev_plot, loadings_2016 = perform_pca_analysis(data_2016)\n",
    "ev_plot.show()\n",
    "\n",
    "# Displaying loadings\n",
    "print(' --- COMPONENT 1 --- ')\n",
    "print(loadings_2014[\"Component 1\"])\n",
    "print(' --- COMPONENT 2 --- ')\n",
    "print(loadings_2014[\"Component 2\"])\n",
    "print(' --- COMPONENT 3 --- ')\n",
    "print(loadings_2014[\"Component 3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6948204-c167-4b74-a39f-0594cf27b4fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2017 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646b138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016 dataset\n",
    "pca_model, ev_plot, loadings_2017 = perform_pca_analysis(data_2017)\n",
    "ev_plot.show()\n",
    "\n",
    "# Displaying loadings\n",
    "print(' --- COMPONENT 1 --- ')\n",
    "print(loadings_2014[\"Component 1\"])\n",
    "print(' --- COMPONENT 2 --- ')\n",
    "print(loadings_2014[\"Component 2\"])\n",
    "print(' --- COMPONENT 3 --- ')\n",
    "print(loadings_2014[\"Component 3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da177f9d-b872-4fcb-b710-ead492ae7731",
   "metadata": {},
   "source": [
    "# 2018 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f7fb0d-4bf0-456e-bde1-34baa56f6924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c36b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016 dataset\n",
    "pca_model, ev_plot, loadings_2018 = perform_pca_analysis(data_2018)\n",
    "ev_plot.show()\n",
    "\n",
    "# Displaying loadings\n",
    "print(' --- COMPONENT 1 --- ')\n",
    "print(loadings_2014[\"Component 1\"])\n",
    "print(' --- COMPONENT 2 --- ')\n",
    "print(loadings_2014[\"Component 2\"])\n",
    "print(' --- COMPONENT 3 --- ')\n",
    "print(loadings_2014[\"Component 3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8823a779-1320-4f39-8237-7fbddbbc3f83",
   "metadata": {},
   "source": [
    "## PCA Explained Variance\n",
    "\n",
    "- The plot shows that the cumulative explained variance increases sharply with the number of components and starts to plateau around 100 components.\n",
    "- This suggests that around 100 components can explain most of the variance in your data, allowing for significant dimensionality reduction from the original number of features (which appears to be around 200).\n",
    "\n",
    "## Loadings for PCA Components (2014 and 2015)\n",
    "\n",
    "### Component 1\n",
    "The top features are related to profitability and earnings:\n",
    "\n",
    "- **EBITDA, EBIT, Operating Income**: These are direct measures of a company's operational efficiency and profitability.\n",
    "- **Gross Profit, Operating Cash Flow**: Indicate the fundamental earnings power of the company.\n",
    "\n",
    "This component seems to capture the overall operational performance of companies.\n",
    "\n",
    "### Component 2\n",
    "The top features focus on various profitability ratios:\n",
    "\n",
    "- **ebitperRevenue, netProfitMargin, pretaxProfitMargin**: These ratios measure the efficiency of profit generation relative to sales and pre-tax earnings.\n",
    "- **Profit Margin, EBITDA Margin**: Further detail on profitability from different accounting perspectives.\n",
    "\n",
    "This component reflects different facets of profitability margins, emphasizing how effectively companies convert sales into profits.\n",
    "\n",
    "### Component 3\n",
    "Features in this component are related to financial structure and valuation:\n",
    "\n",
    "- **Graham Net-Net, Tangible Book Value per Share**: Indicators of potentially undervalued stocks based on asset-based valuation metrics.\n",
    "- **companyEquityMultiplier**: A measure of financial leverage.\n",
    "- **Net Cash/Marketcap, PTB ratio (Price to Book ratio)**: Indicators of financial stability and valuation.\n",
    "\n",
    "This component captures aspects of company valuation and financial position, which can be crucial for assessing investment potential and risk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8704524-c883-4694-a60c-938a55c0ddde",
   "metadata": {},
   "source": [
    "# Now let's make a cluster analisys and see how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f621c6e-ff33-42d5-b0d3-2602976e4dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make clustering\n",
    "def kmeans_clustering_analysis(data, pca_components=2, num_clusters=4):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering on the provided dataset and visualize the clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    - data (DataFrame): The dataset to perform clustering on.\n",
    "    - pca_components (int): Number of PCA components to use for clustering.\n",
    "    - num_clusters (int): Number of clusters to form.\n",
    "    \n",
    "    Returns:\n",
    "    - clusters: The cluster labels for each data point.\n",
    "    - cluster_plot: Matplotlib figure object for the cluster plot.\n",
    "    \"\"\"\n",
    "    # Isolate numeric data for clustering\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(numeric_data)\n",
    "    \n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=pca_components)\n",
    "    pca_data = pca.fit_transform(scaled_data)\n",
    "    \n",
    "    # Determine the optimal number of clusters using the elbow method\n",
    "    wcss = []\n",
    "    for i in range(1, 15):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "        kmeans.fit(pca_data)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, 15), wcss, marker='o')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.title('Elbow Method to Determine Optimal Number of Clusters')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Apply K-means clustering with the chosen number of clusters\n",
    "    kmeans = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    clusters = kmeans.fit_predict(pca_data)\n",
    "\n",
    "    # Calculate silhouette scores\n",
    "    silhouette_avg = silhouette_score(pca_data, clusters)\n",
    "    silhouette_values = silhouette_samples(pca_data, clusters)\n",
    "    \n",
    "    # Add cluster labels to the original data\n",
    "    data['Cluster'] = clusters\n",
    "    \n",
    "    # Visualize the clusters\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(pca_data[:, 0], pca_data[:, 1], c=clusters, cmap='viridis', marker='o')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('K-means Clustering')\n",
    "    plt.colorbar(label='Cluster')\n",
    "    plt.grid(True)\n",
    "    cluster_plot = plt\n",
    "    \n",
    "    return clusters, cluster_plot, silhouette_avg, silhouette_values\n",
    "\n",
    "# Run silhouette\n",
    "def plot_silhouette_scores(clusters, silhouette_values, num_clusters, silhouette_avg):\n",
    "    \"\"\"\n",
    "    Plot silhouette scores for each sample in each cluster.\n",
    "    \n",
    "    Parameters:\n",
    "    - clusters: Cluster labels for each data point.\n",
    "    - silhouette_values: Silhouette values for each data point.\n",
    "    - num_clusters: Number of clusters.\n",
    "    - silhouette_avg: Average silhouette score.\n",
    "    \n",
    "    Returns:\n",
    "    - silhouette_plot: Matplotlib figure object for the silhouette plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    y_lower = 10\n",
    "    for i in range(num_clusters):\n",
    "        ith_cluster_silhouette_values = silhouette_values[clusters == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values)\n",
    "        \n",
    "        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        \n",
    "        y_lower = y_upper + 10\n",
    "\n",
    "    plt.xlabel(\"Silhouette coefficient values\")\n",
    "    plt.ylabel(\"Cluster label\")\n",
    "    plt.title(\"Silhouette plot for the various clusters\")\n",
    "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb0108f-2d38-4f02-ae8c-225274c1c6f5",
   "metadata": {},
   "source": [
    "## Elbow Method Plot\n",
    "\n",
    "This plot helps determine the optimal number of clusters. The \"elbow\" point appears around 3-4 clusters, which suggests that this is a reasonable choice for the number of clusters for the K-means algorithm.\n",
    "\n",
    "## K-means Clustering Plot\n",
    "\n",
    "This plot shows the results of K-means clustering using the first two principal components. Each color represents a different cluster.\n",
    "\n",
    "## Insights from the Clustering Analysis\n",
    "\n",
    "### Optimal Number of Clusters\n",
    "\n",
    "The elbow plot suggests that 3-4 clusters are optimal. You can choose either 3 or 4 based on further domain knowledge or additional validation techniques.\n",
    "\n",
    "### Cluster Visualization\n",
    "\n",
    "- The K-means clustering plot reveals how the data points are grouped in the reduced PCA space.\n",
    "- The distinct separation between clusters suggests that the PCA transformation effectively captures the underlying structure of the data, and K-means clustering identifies meaningful groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7fcb2f-112b-4443-a053-a0a2421f3abf",
   "metadata": {},
   "source": [
    "# Year 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba3622-e91c-42e6-8ef0-2cd8af99643c",
   "metadata": {},
   "source": [
    "## Silhouette Plot and Clustering Analysis\n",
    "\n",
    "### Silhouette Plot:\n",
    "The silhouette plot shows the silhouette coefficient values for each sample in the clusters. The average silhouette score is approximately 0.63, indicating that the clusters are reasonably well-defined.\n",
    "\n",
    "### Conclusions:\n",
    "\n",
    "1. **Cluster Quality**:\n",
    "   - The average silhouette score of 0.63 suggests that the clusters are well-defined and distinct.\n",
    "   - The majority of samples have high silhouette scores, indicating good cohesion within clusters and separation between clusters.\n",
    "\n",
    "2. **Cluster Sizes**:\n",
    "   - Cluster 0 (blue) is the largest, followed by clusters 1 (orange), 2 (green), and 3 (red).\n",
    "   - The thickness of each cluster's silhouette indicates the relative size of each cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01134f12-5570-4bf9-9a5d-a11f1a1992a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, cluster_plot, silhouette_avg, silhouette_values = kmeans_clustering_analysis(data_2014, pca_components=2, num_clusters=4)\n",
    "cluster_plot.show()\n",
    "\n",
    "# Silhouette Score\n",
    "print(f\"Average Silhouette Score: {silhouette_avg}\")\n",
    "plot_silhouette_scores(clusters, silhouette_values, num_clusters=4, silhouette_avg=silhouette_avg)\n",
    "\n",
    "# Display the first few rows of the dataframe with cluster labels\n",
    "data_2014.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff98ac-0afe-4eb3-be14-5a341965bc3c",
   "metadata": {},
   "source": [
    "# Year 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13857c1a-ece9-41c3-b809-6bccf41f2695",
   "metadata": {},
   "source": [
    "## Silhouette Plot and Clustering Analysis\n",
    "\n",
    "### Silhouette Plot:\n",
    "The silhouette plot shows the silhouette coefficient values for each sample in the clusters. The average silhouette score is approximately 0.61, indicating that the clusters are reasonably well-defined.\n",
    "\n",
    "### Conclusions:\n",
    "\n",
    "1. **Cluster Quality**:\n",
    "   - The average silhouette score of 0.61 suggests that the clusters are well-defined and distinct.\n",
    "   - The majority of samples have high silhouette scores, indicating good cohesion within clusters and separation between clusters.\n",
    "\n",
    "2. **Cluster Sizes**:\n",
    "   - Cluster 0 (blue) is the largest, followed by clusters 1 (orange), 2 (green), and 3 (red).\n",
    "   - The thickness of each cluster's silhouette indicates the relative size of each cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc1ff8-aff3-4762-8aa3-99ec1d8d2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, cluster_plot, silhouette_avg, silhouette_values = kmeans_clustering_analysis(data_2015, pca_components=2, num_clusters=4)\n",
    "cluster_plot.show()\n",
    "\n",
    "# Silhouette Score\n",
    "print(f\"Average Silhouette Score: {silhouette_avg}\")\n",
    "plot_silhouette_scores(clusters, silhouette_values, num_clusters=4, silhouette_avg=silhouette_avg)\n",
    "\n",
    "# Display the first few rows of the dataframe with cluster labels\n",
    "data_2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35cc73f-3ab5-4708-b7cd-58c1fd1ff095",
   "metadata": {},
   "source": [
    "# Year 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8702b4-19a4-453c-aef6-a75f2ecb57ab",
   "metadata": {},
   "source": [
    "## Silhouette Plot and Clustering Analysis (2016)\n",
    "\n",
    "### Silhouette Plot:\n",
    "The silhouette plot shows the silhouette coefficient values for each sample in the clusters. The average silhouette score is approximately 0.57, indicating that the clusters are moderately well-defined.\n",
    "\n",
    "### Conclusions:\n",
    "\n",
    "1. **Cluster Quality**:\n",
    "   - The average silhouette score of 0.57 suggests that the clusters are moderately well-defined.\n",
    "   - While many samples have high silhouette scores, indicating good cohesion within clusters, there are also some samples with lower scores, indicating some overlap between clusters.\n",
    "\n",
    "2. **Cluster Sizes**:\n",
    "   - Cluster 2 (green) is the largest, followed by clusters 1 (orange), 3 (red), and 0 (blue).\n",
    "   - The thickness of each cluster's silhouette indicates the relative size of each cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c068e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, cluster_plot, silhouette_avg, silhouette_values = kmeans_clustering_analysis(data_2016, pca_components=2, num_clusters=4)\n",
    "cluster_plot.show()\n",
    "\n",
    "# Silhouette Score\n",
    "print(f\"Average Silhouette Score: {silhouette_avg}\")\n",
    "plot_silhouette_scores(clusters, silhouette_values, num_clusters=4, silhouette_avg=silhouette_avg)\n",
    "\n",
    "# Display the first few rows of the dataframe with cluster labels\n",
    "data_2016.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86366c-fa18-43c6-a6cd-9534194b2641",
   "metadata": {},
   "source": [
    "# Year 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f75ce-c5b7-45aa-a53e-497a8de89e8e",
   "metadata": {},
   "source": [
    "## Silhouette Plot and Clustering Analysis\n",
    "\n",
    "### Silhouette Plot:\n",
    "The silhouette plot shows the silhouette coefficient values for each sample in the clusters. The average silhouette score is approximately 0.60, indicating that the clusters are moderately well-defined.\n",
    "\n",
    "### Conclusions:\n",
    "\n",
    "1. **Cluster Quality**:\n",
    "   - The average silhouette score of 0.60 suggests that the clusters are moderately well-defined.\n",
    "   - While many samples have high silhouette scores, indicating good cohesion within clusters, there are also some samples with lower scores, indicating some overlap between clusters.\n",
    "\n",
    "2. **Cluster Sizes**:\n",
    "   - Cluster 3 (red) is the largest, followed by clusters 2 (green), 1 (orange), and 0 (blue).\n",
    "   - The thickness of each cluster's silhouette indicates the relative size of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, cluster_plot, silhouette_avg, silhouette_values = kmeans_clustering_analysis(data_2017, pca_components=2, num_clusters=4)\n",
    "cluster_plot.show()\n",
    "\n",
    "# Silhouette Score\n",
    "print(f\"Average Silhouette Score: {silhouette_avg}\")\n",
    "plot_silhouette_scores(clusters, silhouette_values, num_clusters=4, silhouette_avg=silhouette_avg)\n",
    "\n",
    "# Display the first few rows of the dataframe with cluster labels\n",
    "data_2017.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a36ce1-aed9-4282-a3c1-156bb78ebab6",
   "metadata": {},
   "source": [
    "# Year 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce527fb3-1b0c-40f5-974a-f59c1291a3f0",
   "metadata": {},
   "source": [
    "## Silhouette Plot and Clustering Analysis\n",
    "\n",
    "### Silhouette Plot:\n",
    "The silhouette plot shows the silhouette coefficient values for each sample in the clusters. The average silhouette score is approximately 0.62, indicating that the clusters are well-defined.\n",
    "\n",
    "### Conclusions:\n",
    "\n",
    "1. **Cluster Quality**:\n",
    "   - The average silhouette score of 0.62 suggests that the clusters are well-defined and distinct.\n",
    "   - The majority of samples have high silhouette scores, indicating good cohesion within clusters and separation between clusters.\n",
    "\n",
    "2. **Cluster Sizes**:\n",
    "   - Cluster 0 (blue) is the largest, followed by clusters 1 (orange), 2 (green), and 3 (red).\n",
    "   - The thickness of each cluster's silhouette indicates the relative size of each cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b05690",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, cluster_plot, silhouette_avg, silhouette_values = kmeans_clustering_analysis(data_2018, pca_components=2, num_clusters=4)\n",
    "cluster_plot.show()\n",
    "\n",
    "# Silhouette Score\n",
    "print(f\"Average Silhouette Score: {silhouette_avg}\")\n",
    "plot_silhouette_scores(clusters, silhouette_values, num_clusters=4, silhouette_avg=silhouette_avg)\n",
    "\n",
    "# Display the first few rows of the dataframe with cluster labels\n",
    "data_2018.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae39bb3-0b2c-4a47-9d31-26264fd4c373",
   "metadata": {},
   "source": [
    "# Overal Conclusions :\n",
    "\n",
    "### Operational Efficiency and Profitability:\n",
    "\n",
    "Component 1 captures the variance related to a company's operational efficiency and profitability. The high loadings on features like EBITDA, EBIT, and Operating Income suggest that these clusters represent different levels of operational performance among companies.\n",
    "\n",
    "### Profitability Ratios:\n",
    "\n",
    "Component 2 focuses on profitability ratios. The clusters separated along this component likely reflect differences in how efficiently companies convert their revenues into profits, considering various profitability margins.\n",
    "\n",
    "### Cluster Interpretation:\n",
    "\n",
    "- **Cluster 0**: This cluster might represent companies with high profitability and strong operational efficiency, as indicated by the high values along both principal components.\n",
    "- **Cluster 1**: Companies in this cluster could have moderate operational performance but better profitability ratios than those in other clusters.\n",
    "- **Cluster 2**: This cluster could represent companies with moderate to low operational performance and profitability ratios.\n",
    "- **Cluster 3**: Companies in this cluster might have the lowest operational performance and profitability ratios among the groups.\n",
    "\n",
    "### Investment Strategy:\n",
    "\n",
    "- Investors can use this clustering analysis to identify groups of companies with similar financial characteristics. For example, clusters with high operational efficiency and profitability might be more attractive investment targets.\n",
    "- Conversely, clusters representing companies with lower performance might be candidates for further investigation or exclusion from investment portfolios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1810416-16e9-4367-b916-a3eb2ea6ef39",
   "metadata": {},
   "source": [
    "# Now Let's see what companies are in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb0021e-6e1d-4c3d-8291-5090aab6388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pca_analysis(data, num_top_features=10):\n",
    "    # Isolate numeric data for PCA\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(numeric_data)\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_data)\n",
    "    \n",
    "    return pca, scaled_data\n",
    "\n",
    "def kmeans_clustering_analysis(scaled_data, pca_components=2, num_clusters=4):\n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=pca_components)\n",
    "    pca_data = pca.fit_transform(scaled_data)\n",
    "    \n",
    "    # Apply K-means clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    clusters = kmeans.fit_predict(pca_data)\n",
    "    \n",
    "    return clusters, pca_data\n",
    "\n",
    "def companies_in_clusters(data, num_clusters=4, pca_components=2):\n",
    "    # Perform PCA analysis and scale the data\n",
    "    pca, scaled_data = perform_pca_analysis(data)\n",
    "    \n",
    "    # Perform K-means clustering\n",
    "    clusters, pca_data = kmeans_clustering_analysis(scaled_data, pca_components, num_clusters)\n",
    "    \n",
    "    # Add cluster labels to the original data\n",
    "    data['Cluster'] = clusters\n",
    "    \n",
    "    # Group companies by their cluster\n",
    "    cluster_groups = data.groupby('Cluster')\n",
    "    \n",
    "    # Create a dictionary to store company names in each cluster\n",
    "    companies_dict = {i: cluster_groups.get_group(i)['Symbol'].tolist() for i in range(num_clusters)}\n",
    "    \n",
    "    return companies_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9263cfdd-a1c0-41e0-a1e1-3cb7ef59d7a3",
   "metadata": {},
   "source": [
    "## Year 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee4e1bd-6449-4934-8a9e-4e86528b5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with the 2014 dataset\n",
    "data_2014 = pd.read_csv('processed_data/clean_df_2014.csv')\n",
    "companies_dict_2014 = companies_in_clusters(data_2014, num_clusters=4, pca_components=2)\n",
    "\n",
    "# Display companies in each cluster for the 2014 dataset\n",
    "for cluster, companies in companies_dict_2014.items():\n",
    "    print(f\"Cluster {cluster}:\")\n",
    "    print(companies)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34297473-098b-4258-bfee-827e80bff414",
   "metadata": {},
   "source": [
    "### Visual Analysis of K-means Clustering Results for 2014\n",
    "\n",
    "Based on the K-means clustering plot, we can observe the following clusters and their potential implications:\n",
    "\n",
    "- **Cluster 0 (Purple)**: This cluster is densely packed on the left side of the plot, indicating that the companies in this cluster share similar financial characteristics that distinguish them significantly from others.\n",
    "- **Cluster 1 (Yellow)**: This cluster is well-separated and occupies a central position, potentially representing companies with balanced characteristics between operational efficiency and profitability.\n",
    "- **Cluster 2 (Green)**: This cluster extends vertically on the far left, indicating a strong influence of the second principal component. Companies here might have distinct financial metrics related to profitability ratios.\n",
    "- **Cluster 3 (Blue)**: This cluster is spread out on the right side and appears to have high values along the first principal component, suggesting strong overall financial health and operational efficiency.\n",
    "\n",
    "### Cluster Composition and Potential Best Cluster\n",
    "Based on the given component loadings and visual observations, we can hypothesize the following about each cluster:\n",
    "\n",
    "- **Cluster 0 (Purple)**: Contains companies like RAD, GIS, BRFS, KHC, etc., which might have unique financial characteristics setting them apart. This cluster might include companies with specific market or sectorial advantages.\n",
    "  \n",
    "- **Cluster 1 (Yellow)**: Includes companies such as VIPS, TAL, NWL, HRL, etc., that might represent a balanced mix of profitability and operational performance. This cluster could be indicative of companies with stable financial performance.\n",
    "\n",
    "- **Cluster 2 (Green)**: Comprising large and well-established companies like PG, KR, PM, KO, etc., this cluster likely represents the best-performing companies in terms of overall financial health and operational efficiency. This cluster could be seen as the \"best\" due to the inclusion of high-performing, large-cap companies.\n",
    "\n",
    "- **Cluster 3 (Blue)**: This cluster consists of companies such as FRPT, COE, MUX, AGFS, etc., and might represent companies with distinct profitability ratios but potentially smaller in size or emerging markets.\n",
    "\n",
    "### Conclusion\n",
    "Visually, **Cluster 2 (Green)** appears to be the best cluster due to the presence of large, well-established companies with strong financial health and operational efficiency. This conclusion is further supported by the high loadings on financial metrics such as EBITDA, EBIT, Operating Income, and various profitability margins.\n",
    "\n",
    "For a more detailed analysis, further investigation into the specific financial metrics and performance of companies within each cluster is recommended. This could involve examining the average values of key financial indicators within each cluster and comparing them against your criteria for identifying the \"best\" companies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db0e8f9-ad37-4425-b9e6-ade4882b62a3",
   "metadata": {},
   "source": [
    "# Let's assess price growth potential, for each cluster by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f5e0c-ae54-4fac-9d3e-876396ae809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cluster_performance(data, companies_clusters):\n",
    "    # Create a dictionary to hold performance data for each cluster\n",
    "    performance_dict = {}\n",
    "    \n",
    "    title = f'{str(round(data['Year'][0] + 1))} PRICE VAR [%]'\n",
    "    \n",
    "    for cluster, companies in companies_clusters.items():\n",
    "        cluster_data = data[data['Symbol'].isin(companies)]\n",
    "        mean_var = cluster_data[title].mean()\n",
    "        median_var = cluster_data[title].median()\n",
    "        std_var = cluster_data[title].std()\n",
    "        performance_dict[cluster] = {'Mean': mean_var, 'Median': median_var, 'Standard Deviation': std_var}\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='Cluster', y=title, data=data)\n",
    "    plt.title(f'{title} Distribution by Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return performance_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f2a45-99f4-4b78-be35-4ecf295a9464",
   "metadata": {},
   "source": [
    "## 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f857233-5f21-4f21-9963-179e1f33f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance for each cluster\n",
    "cluster_performance = analyze_cluster_performance(data_2014, companies_dict_2014)\n",
    "\n",
    "# Display the performance\n",
    "cluster_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a46eed-8c70-463b-9ad8-abb083354f61",
   "metadata": {},
   "source": [
    "## 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ab24aa-8367-49a6-b4d2-300af44282d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_dict_2015 = companies_in_clusters(data_2015, num_clusters=4, pca_components=2)\n",
    "\n",
    "# Calculate performance for each cluster\n",
    "cluster_performance = analyze_cluster_performance(data_2015, companies_dict_2015)\n",
    "\n",
    "# Display the performance\n",
    "cluster_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b9243-c051-4aa6-8530-32c9c4c1620f",
   "metadata": {},
   "source": [
    "## 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd67088",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_dict_2016 = companies_in_clusters(data_2016, num_clusters=4, pca_components=2)\n",
    "\n",
    "# Calculate performance for each cluster\n",
    "cluster_performance = analyze_cluster_performance(data_2016, companies_dict_2016)\n",
    "\n",
    "# Display the performance\n",
    "cluster_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4bbb3a-9f70-4d7d-bb33-37340209b3d9",
   "metadata": {},
   "source": [
    "## 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a955e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_dict_2017 = companies_in_clusters(data_2017, num_clusters=4, pca_components=2)\n",
    "\n",
    "# Calculate performance for each cluster\n",
    "cluster_performance = analyze_cluster_performance(data_2017, companies_dict_2017)\n",
    "\n",
    "# Display the performance\n",
    "cluster_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600e3fa-f8c0-4639-89ac-f02403ba343e",
   "metadata": {},
   "source": [
    "## 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd7a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_dict_2018 = companies_in_clusters(data_2018, num_clusters=4, pca_components=2)\n",
    "\n",
    "# Calculate performance for each cluster\n",
    "cluster_performance = analyze_cluster_performance(data_2018, companies_dict_2018)\n",
    "\n",
    "# Display the performance\n",
    "cluster_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd20869f-e6da-4a36-9766-bb00bd965bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
